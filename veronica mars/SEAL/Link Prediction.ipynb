{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Create the RDF Triples w/o time or reification\n",
    "    Here triples are read in from two seperate files -- one containing the triples to be tested\n",
    "    the other containing the rest of the triples\n",
    "'''\n",
    "import rdflib\n",
    "import xlrd\n",
    "import pandas\n",
    "from rdflib import URIRef, Literal, BNode,Namespace\n",
    "\n",
    "data = { # similar format for how you made your pandas data fram\n",
    "    \"subject\": [],\n",
    "    \"predicate\": [],\n",
    "    \"object\" : [],\n",
    "}\n",
    "SS_name = \"data/VMars Triples SEAL.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "\n",
    "for l in range(wb.nsheets):\n",
    "    sheet = wb.sheet_by_index(l)\n",
    "    for i in range(1, sheet.nrows): # skip the first header line\n",
    "        s = sheet.cell_value(i, 0)\n",
    "        p = sheet.cell_value(i, 1)\n",
    "        o = sheet.cell_value(i, 2)\n",
    "\n",
    "        if not s or not p or not o: # if any are empty then no triple\n",
    "            continue\n",
    "            \n",
    "        data[\"subject\"].append(s)\n",
    "        data[\"predicate\"].append(p)\n",
    "        data[\"object\"].append(o)\n",
    "\n",
    "\n",
    "data_processed = {\n",
    "    'subject':[],'predicate':[],'object': []\n",
    "}\n",
    "ugly_token = {\n",
    "    ' ': '_',\n",
    "    '\"': '',\n",
    "}\n",
    "\n",
    "# replace all ugly tokens and copy to new data structure\n",
    "for x in data.keys(): #x is subj,obj,pred\n",
    "    for item in data[x]:\n",
    "        if type(item)!=str:\n",
    "            data_processed[x].append(item)\n",
    "            continue\n",
    "        new_token=item       \n",
    "        for k in ugly_token:\n",
    "            new_token = new_token.strip()\n",
    "            new_token=new_token.replace(k, ugly_token[k]) \n",
    "            new_token=new_token.lower()\n",
    "        data_processed[x].append(new_token)            \n",
    "\n",
    "g =[]\n",
    "\n",
    "for i in range(len(data['subject'])):\n",
    "    s = data_processed['subject'][i]\n",
    "    p = data_processed['predicate'][i]\n",
    "    o=data_processed['object'][i]\n",
    "    g.append((s, p, o))\n",
    "\n",
    "\n",
    "\n",
    "SS_name = \"data/node_numbers.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(0)\n",
    "nodelabel={}\n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    node= sheet.cell_value(i, 0)\n",
    "    if type(node) == str:\n",
    "        node=node.lower()\n",
    "    nodenumber= int(sheet.cell_value(i, 1))\n",
    "    nodelabel[node]=nodenumber\n",
    "ent_id =nodelabel\n",
    "\n",
    "\n",
    "def process_entity_name(name, ugly_token= {' ': '_', '\"': '',}):\n",
    "    if type(name) != str:\n",
    "        return name\n",
    "    new_token = name\n",
    "    for k in ugly_token:\n",
    "        new_token = new_token.strip()\n",
    "        new_token=new_token.replace(k, ugly_token[k])\n",
    "        new_token=new_token.lower()\n",
    "    return new_token\n",
    "\n",
    "\n",
    "ent_id = {process_entity_name(key): ent_id[key] for key in ent_id}\n",
    "\n",
    "#print(g)\n",
    "SEAL_data = [] # list which will contain string indicating edges\n",
    "for s,p,o in g: # this will get all the triples in g\n",
    "    if s==o:\n",
    "        continue\n",
    "    new_edge = \"{} {}\".format(ent_id[s], ent_id[o])\n",
    "    if new_edge in SEAL_data:\n",
    "         continue\n",
    "    SEAL_data.append(new_edge)\n",
    "\n",
    "#with open(\"data/VMARS_train_data.txt\", \"w\") as output:\n",
    "#    output.write('\\n'.join(SEAL_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and output the results from link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ben', 'norris_clayton', '0.99')\n",
      "('veronica_mars', 'logan_echolls', '1.00')\n",
      "('cliff_mccormack', 'keith_mars', '0.98')\n",
      "('logan_echolls', 'veronica_mars', '1.00')\n",
      "('holly_mills', 'casey_gant', '0.97')\n",
      "('luke_haldeman', 'troy_vandegraff', '0.97')\n",
      "('luke_haldeman', 'logan_echolls', '0.96')\n",
      "('bone_hamilton', 'upper_class', '0.84')\n",
      "('hamilton_cho', 'lower_class', '0.93')\n",
      "('aaron_echolls', 'upper_class', '0.85')\n",
      "('veronica_mars', 'lower_class', '0.95')\n",
      "('celeste_kane', 'upper_class', '0.97')\n",
      "('lilly_kane', 'upper_class', '0.97')\n",
      "('hans', 'lower_class', '0.52')\n",
      "('mandy', 'lower_class', '0.70')\n",
      "('troy_vandegraff', 'upper_class', '0.96')\n",
      "('wallace_fennel', 'lower_class', '0.78')\n",
      "('chardo_navarro', 'lower_class', '0.16')\n",
      "('leticia_navarro', 'lower_class', '0.20')\n",
      "('sean_friedrich', 'lower_class', '0.73')\n",
      "('case13', 'missing_person', '0.65')\n",
      "('case18', 'bomb_threat', '0.77')\n",
      "('case10', 'stolen_money', '0.55')\n",
      "('case17', 'harassment', '0.94')\n",
      "('case4', 'money_scam', '0.05')\n",
      "('case9', 'exposing_the_cult', '0.60')\n",
      "('case15', 'missing_person', '0.70')\n",
      "('case5', 'drug_trafficking', '0.28')\n",
      "('case6', 'rigged_election', '0.18')\n",
      "('perpetrator', 'sean_friedrich', '0.84')\n",
      "('perpetrator', 'kimmy', '0.54')\n",
      "('perpetrator', 'aaron_echolls', '0.37')\n",
      "('perpetrator', 'bryce_hamilton', '0.92')\n",
      "('perpetrator', 'jim_cho', '0.37')\n",
      "('perpetrator', 'mrs_gant', '0.96')\n",
      "('perpetrator', 'chuck_rook', '0.33')\n",
      "('perpetrator', 'chardo_navarro', '0.49')\n",
      "('perpetrator', 'catherina_lenova', '0.47')\n"
     ]
    }
   ],
   "source": [
    "# 50% training vs testing\n",
    "\n",
    "#map predictions to edges and nodenames\n",
    "prediction_data=[]\n",
    "with open(\"data/VMARS_test_data_50_pred.txt\",\"r\") as f:\n",
    "    prediction_data = f.read().splitlines()\n",
    "\n",
    "prediction_data = [x.split(' ') for x in prediction_data]\n",
    "\n",
    "id_ent = {ent_id[key]: key for key in ent_id}\n",
    "\n",
    "prediction_data = list(map(lambda x: (id_ent[int(x[0])], id_ent[int(x[1])], x[2]), prediction_data))\n",
    "\n",
    "for x in prediction_data:\n",
    "    print(x)\n",
    "\n",
    "probs=[]\n",
    "for x in prediction_data:\n",
    "    probs.append(x[2])\n",
    "#with open(\"50_pred.txt\", \"w\") as output:\n",
    "#    output.write('\\n'.join(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('case3', 'absent_father', '0.70')\n",
      "('keith_mars', 'lower_class', '0.98')\n",
      "('dick_casablancas', 'upper_class', '0.89')\n",
      "('meg_manning', 'veronica_mars', '0.98')\n",
      "('perpetrator', 'aaron_echolls', '0.71')\n"
     ]
    }
   ],
   "source": [
    "#10%\n",
    "\n",
    "#map predictions to edges and nodenames\n",
    "prediction_data=[]\n",
    "with open(\"data/VMARS_test_data_10_pred.txt\",\"r\") as f:\n",
    "    prediction_data = f.read().splitlines()\n",
    "\n",
    "prediction_data = [x.split(' ') for x in prediction_data]\n",
    "\n",
    "id_ent = {ent_id[key]: key for key in ent_id}\n",
    "\n",
    "prediction_data = list(map(lambda x: (id_ent[int(x[0])], id_ent[int(x[1])], x[2]), prediction_data))\n",
    "\n",
    "for x in prediction_data:\n",
    "    print(x)\n",
    "probs=[]\n",
    "for x in prediction_data:\n",
    "    probs.append(x[2])\n",
    "with open(\"10_pred.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('case18', 'bomb_threat', '0.76')\n",
      "('case14', 'statutory_rape', '0.97')\n",
      "('case4', 'money_scam', '0.05')\n",
      "('case10', 'stolen_money', '0.56')\n",
      "('perpetrator', 'pete', '0.47')\n",
      "('perpetrator', 'grant_winters', '0.72')\n",
      "('perpetrator', 'julia_smith', '0.51')\n",
      "('perpetrator', 'aaron_echolls', '0.68')\n",
      "('cliff_mccormack', 'keith_mars', '0.98')\n",
      "('ben', 'norris_clayton', '0.99')\n",
      "('veronica_mars', 'casey_gant', '0.94')\n",
      "('wallace_fennel', 'lower_class', '0.96')\n",
      "('caitlin_ford', 'upper_class', '0.96')\n",
      "('connor_larkin', 'upper_class', '0.92')\n",
      "('hans', 'lower_class', '0.78')\n",
      "('celeste_kane', 'upper_class', '0.98')\n",
      "('duncan_kane', 'upper_class', '0.98')\n"
     ]
    }
   ],
   "source": [
    "#25%\n",
    "\n",
    "#map predictions to edges and nodenames\n",
    "prediction_data=[]\n",
    "with open(\"dataVMARS_test_data_25_pred.txt\",\"r\") as f:\n",
    "    prediction_data = f.read().splitlines()\n",
    "\n",
    "prediction_data = [x.split(' ') for x in prediction_data]\n",
    "\n",
    "id_ent = {ent_id[key]: key for key in ent_id}\n",
    "\n",
    "prediction_data = list(map(lambda x: (id_ent[int(x[0])], id_ent[int(x[1])], x[2]), prediction_data))\n",
    "\n",
    "for x in prediction_data:\n",
    "    print(x)\n",
    "probs=[]\n",
    "for x in prediction_data:\n",
    "    probs.append(x[2])\n",
    "with open(\"25_pred.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('case4', 'money_scam', '0.08')\n",
      "('case13', 'missing_person', '0.63')\n",
      "('case18', 'bomb_threat', '0.83')\n",
      "('case5', 'drug_trafficking', '0.34')\n",
      "('case16', 'stolen_mascot', '0.80')\n",
      "('case1', 'murder', '0.72')\n",
      "('case17', 'harassment', '0.96')\n",
      "('case10', 'stolen_money', '0.66')\n",
      "('case3', 'absent_father', '0.60')\n",
      "('case15', 'missing_person', '0.68')\n",
      "('case12', 'fake_ids', '0.91')\n",
      "('case6', 'rigged_election', '0.21')\n",
      "('case19', 'missing_dogs', '0.18')\n",
      "('logan_echolls', 'upper_class', '0.54')\n",
      "('connor_larkin', 'upper_class', '0.81')\n",
      "('dick_casablancas', 'upper_class', '0.60')\n",
      "('veronica_mars', 'lower_class', '0.84')\n",
      "('caitlin_ford', 'upper_class', '0.61')\n",
      "('jim_cho', 'lower_class', '0.86')\n",
      "('wallace_fennel', 'lower_class', '0.64')\n",
      "('mandy', 'lower_class', '0.68')\n",
      "('keith_mars', 'lower_class', '0.77')\n",
      "('luke_haldeman', 'upper_class', '0.66')\n",
      "('lilly_kane', 'upper_class', '0.53')\n",
      "('weevil_navarro', 'lower_class', '0.63')\n",
      "('chardo_navarro', 'lower_class', '0.68')\n",
      "('leticia_navarro', 'lower_class', '0.74')\n",
      "('hamilton_cho', 'lower_class', '0.92')\n",
      "('sabrina_fuller', 'upper_class', '0.61')\n",
      "('sean_friedrich', 'lower_class', '0.81')\n",
      "(\"casey's_grandmother\", 'upper_class', '0.98')\n",
      "('meg_manning', 'upper_class', '0.76')\n",
      "('lianne_mars', 'lower_class', '0.81')\n",
      "('cliff_mccormack', 'veronica_mars', '0.97')\n",
      "('kimmy', 'meg_manning', '0.99')\n",
      "('troy_vandegraff', 'logan_echolls', '1.00')\n",
      "('meg_manning', 'veronica_mars', '0.99')\n",
      "('ben', 'norris_clayton', '0.99')\n",
      "('luke_haldeman', 'troy_vandegraff', '0.97')\n",
      "('veronica_mars', 'logan_echolls', '1.00')\n",
      "('norris_clayton', 'ben', '0.99')\n",
      "('luke_haldeman', 'logan_echolls', '0.96')\n",
      "('holly_mills', 'casey_gant', '0.98')\n",
      "('logan_echolls', 'veronica_mars', '1.00')\n",
      "('perpetrator', 'mr_gant', '0.81')\n",
      "('perpetrator', 'sean_friedrich', '0.88')\n",
      "('perpetrator', 'madison_sinclair', '0.64')\n",
      "('perpetrator', 'grant_winters', '0.49')\n",
      "('perpetrator', 'rick', '0.91')\n",
      "('perpetrator', 'chuck_rook', '0.37')\n",
      "('perpetrator', 'gabe', '0.76')\n",
      "('perpetrator', 'aaron_echolls', '0.31')\n",
      "('perpetrator', 'troy_vandegraff', '0.63')\n",
      "('perpetrator', 'kimmy', '0.67')\n",
      "('perpetrator', 'bryce_hamilton', '0.92')\n",
      "('perpetrator', 'pete', '0.62')\n",
      "('perpetrator', 'jim_cho', '0.32')\n",
      "('perpetrator', 'mrs_gant', '0.81')\n"
     ]
    }
   ],
   "source": [
    "#75%\n",
    "\n",
    "#map predictions to edges and nodenames\n",
    "prediction_data=[]\n",
    "with open(\"data/VMARS_test_data_75_pred.txt\",\"r\") as f:\n",
    "    prediction_data = f.read().splitlines()\n",
    "\n",
    "prediction_data = [x.split(' ') for x in prediction_data]\n",
    "\n",
    "id_ent = {ent_id[key]: key for key in ent_id}\n",
    "\n",
    "prediction_data = list(map(lambda x: (id_ent[int(x[0])], id_ent[int(x[1])], x[2]), prediction_data))\n",
    "\n",
    "for x in prediction_data:\n",
    "    print(x)\n",
    "probs=[]\n",
    "for x in prediction_data:\n",
    "    probs.append(x[2])\n",
    "with open(\"75_pred.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code blocks split the dataset into testing and training. The spreadsheet 'Seperate Triples.xlsx' contain the all the triples from which we want to generate the training data from. We use these to generate the training and testing data for SEAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import xlrd\n",
    "import pandas\n",
    "from rdflib import URIRef, Literal, BNode,Namespace\n",
    "\n",
    "data = { # similar format for how you made your pandas data fram\n",
    "    \"subject\": [],\n",
    "    \"predicate\": [],\n",
    "    \"object\" : [],\n",
    "}\n",
    "SS_name = \"Seperate Triples.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(0) #perpetrator sheet\n",
    "#for l in range(wb.nsheets):\n",
    "   \n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    s = sheet.cell_value(i, 0)\n",
    "    p = sheet.cell_value(i, 1)\n",
    "    o = sheet.cell_value(i, 2)\n",
    "\n",
    "    if not s or not p or not o: # if any are empty then no triple\n",
    "        continue\n",
    "            \n",
    "    data[\"subject\"].append(s)\n",
    "    data[\"predicate\"].append(p)\n",
    "    data[\"object\"].append(o)\n",
    "\n",
    "\n",
    "data_processed = {\n",
    "    'subject':[],'predicate':[],'object': []\n",
    "}\n",
    "ugly_token = {\n",
    "    ' ': '_',\n",
    "    '\"': '',\n",
    "}\n",
    "\n",
    "# replace all ugly tokens and copy to new data structure\n",
    "for x in data.keys(): #x is subj,obj,pred\n",
    "    for item in data[x]:\n",
    "        if type(item)!=str:\n",
    "            data_processed[x].append(item)\n",
    "            continue\n",
    "        new_token=item       \n",
    "        for k in ugly_token:\n",
    "            new_token = new_token.strip()\n",
    "            new_token=new_token.replace(k, ugly_token[k]) \n",
    "            new_token=new_token.lower()\n",
    "        data_processed[x].append(new_token)            \n",
    "\n",
    "g =[]\n",
    "\n",
    "# Read all the other triples into g.\n",
    "for i in range(len(data['subject'])):\n",
    "    s = data_processed['subject'][i]\n",
    "    p = data_processed['predicate'][i]\n",
    "    o=data_processed['object'][i]\n",
    "    g.append((s, p, o))\n",
    "\n",
    "\n",
    "\n",
    "SS_name = \"node_numbers.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(0)\n",
    "nodelabel={}\n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    node= sheet.cell_value(i, 0)\n",
    "    if type(node) == str:\n",
    "        node=node.lower()\n",
    "    nodenumber= int(sheet.cell_value(i, 1))\n",
    "    nodelabel[node]=nodenumber\n",
    "ent_id =nodelabel\n",
    "\n",
    "\n",
    "def process_entity_name(name, ugly_token= {' ': '_', '\"': '',}):\n",
    "    if type(name) != str:\n",
    "        return name\n",
    "    new_token = name\n",
    "    for k in ugly_token:\n",
    "        new_token = new_token.strip()\n",
    "        new_token=new_token.replace(k, ugly_token[k])\n",
    "        new_token=new_token.lower()\n",
    "    return new_token\n",
    "\n",
    "\n",
    "ent_id = {process_entity_name(key): ent_id[key] for key in ent_id}\n",
    "\n",
    "#print(g)\n",
    "Perpetrator_data = [] # list which will contain string indicating edges\n",
    "for s,p,o in g: # this will get all the triples in g\n",
    "    if s==o:\n",
    "        continue\n",
    "    new_edge = \"{} {}\".format(ent_id[s], ent_id[o])\n",
    "    if new_edge in Perpetrator_data:\n",
    "         continue\n",
    "    Perpetrator_data.append(new_edge)\n",
    "'''\n",
    "with open(\"Perpetator_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(Perpetrator_data))\n",
    "'''\n",
    "# split testing and training\n",
    "'''\n",
    "    Change p to choose what proportion of the relevent predicates are taken for testing\n",
    "'''\n",
    "from random import randint\n",
    "train_data = list(Perpetrator_data) # copy of seal_data\n",
    "test_data = []\n",
    "p = 0.75 # proportion of perp data to turn into test data\n",
    "num_test_entries = int(p * len(train_data))\n",
    "for i in range(num_test_entries):\n",
    "    edge_i = randint(0, len(train_data)-1)\n",
    "    test_data.append(train_data.pop(edge_i)) # removes chosen element from seal_data and puts it into test_data\n",
    "#write train_data to training file and test_data to testing file\n",
    "with open(\"perp_test_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(test_data))\n",
    "\n",
    "with open(\"perp_train_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import xlrd\n",
    "import pandas\n",
    "from rdflib import URIRef, Literal, BNode,Namespace\n",
    "\n",
    "data = { # similar format for how you made your pandas data fram\n",
    "    \"subject\": [],\n",
    "    \"predicate\": [],\n",
    "    \"object\" : [],\n",
    "}\n",
    "# Seperate triples contains all the triples for testing\n",
    "SS_name = \"Seperate Triples.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(1) #case classification sheet\n",
    "#for l in range(wb.nsheets):\n",
    "   \n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    s = sheet.cell_value(i, 0)\n",
    "    p = sheet.cell_value(i, 1)\n",
    "    o = sheet.cell_value(i, 2)\n",
    "\n",
    "    if not s or not p or not o: # if any are empty then no triple\n",
    "        continue\n",
    "            \n",
    "    data[\"subject\"].append(s)\n",
    "    data[\"predicate\"].append(p)\n",
    "    data[\"object\"].append(o)\n",
    "\n",
    "\n",
    "data_processed = {\n",
    "    'subject':[],'predicate':[],'object': []\n",
    "}\n",
    "ugly_token = {\n",
    "    ' ': '_',\n",
    "    '\"': '',\n",
    "}\n",
    "\n",
    "# replace all ugly tokens and copy to new data structure\n",
    "for x in data.keys(): #x is subj,obj,pred\n",
    "    for item in data[x]:\n",
    "        if type(item)!=str:\n",
    "            data_processed[x].append(item)\n",
    "            continue\n",
    "        new_token=item       \n",
    "        for k in ugly_token:\n",
    "            new_token = new_token.strip()\n",
    "            new_token=new_token.replace(k, ugly_token[k]) \n",
    "            new_token=new_token.lower()\n",
    "        data_processed[x].append(new_token)            \n",
    "\n",
    "g =[]\n",
    "\n",
    "for i in range(len(data['subject'])):\n",
    "    s = data_processed['subject'][i]\n",
    "    p = data_processed['predicate'][i]\n",
    "    o=data_processed['object'][i]\n",
    "    g.append((s, p, o))\n",
    "\n",
    "\n",
    "# contains mapping of entities to distinct positive integers\n",
    "SS_name = \"node_numbers.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(0)\n",
    "nodelabel={}\n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    node= sheet.cell_value(i, 0)\n",
    "    if type(node) == str:\n",
    "        node=node.lower()\n",
    "    nodenumber= int(sheet.cell_value(i, 1))\n",
    "    nodelabel[node]=nodenumber\n",
    "ent_id =nodelabel\n",
    "\n",
    "\n",
    "def process_entity_name(name, ugly_token= {' ': '_', '\"': '',}):\n",
    "    if type(name) != str:\n",
    "        return name\n",
    "    new_token = name\n",
    "    for k in ugly_token:\n",
    "        new_token = new_token.strip()\n",
    "        new_token=new_token.replace(k, ugly_token[k])\n",
    "        new_token=new_token.lower()\n",
    "    return new_token\n",
    "\n",
    "\n",
    "ent_id = {process_entity_name(key): ent_id[key] for key in ent_id}\n",
    "\n",
    "#print(g)\n",
    "case_data = [] # list which will contain string indicating edges\n",
    "for s,p,o in g: # this will get all the triples in g\n",
    "    if s==o:\n",
    "        continue\n",
    "    new_edge = \"{} {}\".format(ent_id[s], ent_id[o])\n",
    "    if new_edge in case_data:\n",
    "         continue\n",
    "    case_data.append(new_edge)\n",
    "'''\n",
    "with open(\"case_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(case_data))\n",
    "'''\n",
    "# split testing and training\n",
    "from random import randint\n",
    "train_data = list(case_data) # copy of seal_data\n",
    "test_data = []\n",
    "p = 0.75 # proportion of perp data to turn into test data\n",
    "num_test_entries = int(p * len(train_data))\n",
    "for i in range(num_test_entries):\n",
    "    edge_i = randint(0, len(train_data)-1)\n",
    "    test_data.append(train_data.pop(edge_i)) # removes chosen element from seal_data and puts it into test_data\n",
    "#write train_data to training file and test_data to testing file\n",
    "with open(\"case_test_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(test_data))\n",
    "\n",
    "with open(\"case_train_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rdflib\n",
    "import xlrd\n",
    "import pandas\n",
    "from rdflib import URIRef, Literal, BNode,Namespace\n",
    "\n",
    "data = { # similar format for how you made your pandas data fram\n",
    "    \"subject\": [],\n",
    "    \"predicate\": [],\n",
    "    \"object\" : [],\n",
    "}\n",
    "SS_name = \"Seperate Triples.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(2) #financial status sheet\n",
    "#for l in range(wb.nsheets):\n",
    "   \n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    s = sheet.cell_value(i, 0)\n",
    "    p = sheet.cell_value(i, 1)\n",
    "    o = sheet.cell_value(i, 2)\n",
    "\n",
    "    if not s or not p or not o: # if any are empty then no triple\n",
    "        continue\n",
    "            \n",
    "    data[\"subject\"].append(s)\n",
    "    data[\"predicate\"].append(p)\n",
    "    data[\"object\"].append(o)\n",
    "\n",
    "\n",
    "data_processed = {\n",
    "    'subject':[],'predicate':[],'object': []\n",
    "}\n",
    "ugly_token = {\n",
    "    ' ': '_',\n",
    "    '\"': '',\n",
    "}\n",
    "\n",
    "# replace all ugly tokens and copy to new data structure\n",
    "for x in data.keys(): #x is subj,obj,pred\n",
    "    for item in data[x]:\n",
    "        if type(item)!=str:\n",
    "            data_processed[x].append(item)\n",
    "            continue\n",
    "        new_token=item       \n",
    "        for k in ugly_token:\n",
    "            new_token = new_token.strip()\n",
    "            new_token=new_token.replace(k, ugly_token[k]) \n",
    "            new_token=new_token.lower()\n",
    "        data_processed[x].append(new_token)            \n",
    "\n",
    "g =[]\n",
    "\n",
    "for i in range(len(data['subject'])):\n",
    "    s = data_processed['subject'][i]\n",
    "    p = data_processed['predicate'][i]\n",
    "    o=data_processed['object'][i]\n",
    "    g.append((s, p, o))\n",
    "\n",
    "\n",
    "\n",
    "SS_name = \"node_numbers.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(0)\n",
    "nodelabel={}\n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    node= sheet.cell_value(i, 0)\n",
    "    if type(node) == str:\n",
    "        node=node.lower()\n",
    "    nodenumber= int(sheet.cell_value(i, 1))\n",
    "    nodelabel[node]=nodenumber\n",
    "ent_id =nodelabel\n",
    "\n",
    "\n",
    "def process_entity_name(name, ugly_token= {' ': '_', '\"': '',}):\n",
    "    if type(name) != str:\n",
    "        return name\n",
    "    new_token = name\n",
    "    for k in ugly_token:\n",
    "        new_token = new_token.strip()\n",
    "        new_token=new_token.replace(k, ugly_token[k])\n",
    "        new_token=new_token.lower()\n",
    "    return new_token\n",
    "\n",
    "\n",
    "ent_id = {process_entity_name(key): ent_id[key] for key in ent_id}\n",
    "\n",
    "#print(g)\n",
    "finance_data = [] # list which will contain string indicating edges\n",
    "for s,p,o in g: # this will get all the triples in g\n",
    "    if s==o:\n",
    "        continue\n",
    "    new_edge = \"{} {}\".format(ent_id[s], ent_id[o])\n",
    "    if new_edge in finance_data:\n",
    "         continue\n",
    "    finance_data.append(new_edge)\n",
    "'''\n",
    "with open(\"finance_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(finance_data))\n",
    "'''\n",
    "# split testing and training\n",
    "from random import randint\n",
    "train_data = list(finance_data) # copy of seal_data\n",
    "test_data = []\n",
    "p = 0.75 # proportion of perp data to turn into test data\n",
    "num_test_entries = int(p * len(train_data))\n",
    "for i in range(num_test_entries):\n",
    "    edge_i = randint(0, len(train_data)-1)\n",
    "    test_data.append(train_data.pop(edge_i)) # removes chosen element from seal_data and puts it into test_data\n",
    "#write train_data to training file and test_data to testing file\n",
    "with open(\"finance_test_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(test_data))\n",
    "\n",
    "with open(\"finance_train_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Create the RDF Triples w/o time or reification\n",
    "'''\n",
    "import rdflib\n",
    "import xlrd\n",
    "import pandas\n",
    "from rdflib import URIRef, Literal, BNode,Namespace\n",
    "\n",
    "data = { # similar format for how you made your pandas data fram\n",
    "    \"subject\": [],\n",
    "    \"predicate\": [],\n",
    "    \"object\" : [],\n",
    "}\n",
    "SS_name = \"Seperate Triples.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(3) #friend sheet\n",
    "#for l in range(wb.nsheets):\n",
    "   \n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    s = sheet.cell_value(i, 0)\n",
    "    p = sheet.cell_value(i, 1)\n",
    "    o = sheet.cell_value(i, 2)\n",
    "\n",
    "    if not s or not p or not o: # if any are empty then no triple\n",
    "        continue\n",
    "            \n",
    "    data[\"subject\"].append(s)\n",
    "    data[\"predicate\"].append(p)\n",
    "    data[\"object\"].append(o)\n",
    "\n",
    "\n",
    "data_processed = {\n",
    "    'subject':[],'predicate':[],'object': []\n",
    "}\n",
    "ugly_token = {\n",
    "    ' ': '_',\n",
    "    '\"': '',\n",
    "}\n",
    "\n",
    "# replace all ugly tokens and copy to new data structure\n",
    "for x in data.keys(): #x is subj,obj,pred\n",
    "    for item in data[x]:\n",
    "        if type(item)!=str:\n",
    "            data_processed[x].append(item)\n",
    "            continue\n",
    "        new_token=item       \n",
    "        for k in ugly_token:\n",
    "            new_token = new_token.strip()\n",
    "            new_token=new_token.replace(k, ugly_token[k]) \n",
    "            new_token=new_token.lower()\n",
    "        data_processed[x].append(new_token)            \n",
    "\n",
    "g =[]\n",
    "\n",
    "for i in range(len(data['subject'])):\n",
    "    s = data_processed['subject'][i]\n",
    "    p = data_processed['predicate'][i]\n",
    "    o=data_processed['object'][i]\n",
    "    g.append((s, p, o))\n",
    "\n",
    "\n",
    "\n",
    "SS_name = \"node_numbers.xlsx\"\n",
    "wb = xlrd.open_workbook(SS_name)\n",
    "sheet = wb.sheet_by_index(0)\n",
    "nodelabel={}\n",
    "for i in range(0, sheet.nrows): # skip the first header line\n",
    "    node= sheet.cell_value(i, 0)\n",
    "    if type(node) == str:\n",
    "        node=node.lower()\n",
    "    nodenumber= int(sheet.cell_value(i, 1))\n",
    "    nodelabel[node]=nodenumber\n",
    "ent_id =nodelabel\n",
    "\n",
    "\n",
    "def process_entity_name(name, ugly_token= {' ': '_', '\"': '',}):\n",
    "    if type(name) != str:\n",
    "        return name\n",
    "    new_token = name\n",
    "    for k in ugly_token:\n",
    "        new_token = new_token.strip()\n",
    "        new_token=new_token.replace(k, ugly_token[k])\n",
    "        new_token=new_token.lower()\n",
    "    return new_token\n",
    "\n",
    "\n",
    "ent_id = {process_entity_name(key): ent_id[key] for key in ent_id}\n",
    "\n",
    "#print(g)\n",
    "friend_data = [] # list which will contain string indicating edges\n",
    "for s,p,o in g: # this will get all the triples in g\n",
    "    if s==o:\n",
    "        continue\n",
    "    new_edge = \"{} {}\".format(ent_id[s], ent_id[o])\n",
    "    if new_edge in friend_data:\n",
    "         continue\n",
    "    friend_data.append(new_edge)\n",
    "'''\n",
    "with open(\"friend_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(friend_data))\n",
    "'''\n",
    "# split testing and training\n",
    "from random import randint\n",
    "train_data = list(friend_data) # copy of seal_data\n",
    "test_data = []\n",
    "p = 0.75 # proportion of perp data to turn into test data\n",
    "num_test_entries = int(p * len(train_data))\n",
    "for i in range(num_test_entries):\n",
    "    edge_i = randint(0, len(train_data)-1)\n",
    "    test_data.append(train_data.pop(edge_i)) # removes chosen element from seal_data and puts it into test_data\n",
    "#write train_data to training file and test_data to testing file\n",
    "with open(\"friend_test_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(test_data))\n",
    "\n",
    "with open(\"friend_train_data.txt\", \"w\") as output:\n",
    "    output.write('\\n'.join(train_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
